{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import skimage\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "datasetsPath = \"./datasets\"\n",
    "\n",
    "print(\"Path to dataset files:\", datasetsPath)\n",
    "\n",
    "labels = ['pneumonia', 'normal']\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data (dir_relative_path):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        currentPath = os.path.join(datasetsPath, dir_relative_path, label)\n",
    "        print(\"Working inside folder: \", currentPath)\n",
    "        class_number = labels.index(label)\n",
    "\n",
    "        for image in os.listdir(currentPath):\n",
    "            try:\n",
    "                if image.endswith('.DS_Store'):\n",
    "                    continue\n",
    "\n",
    "                image_path = os.path.join(currentPath, image)\n",
    "                image_file = tf.io.read_file(image_path)\n",
    "                \n",
    "                image_arr = tf.image.decode_image(image_file, channels=1)\n",
    "                                \n",
    "                resized_arr = tf.image.resize(image_arr, [image_size, image_size])\n",
    "\n",
    "                data.append([resized_arr, class_number])\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "    return np.array(data, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Import the necessary packages and resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D,Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1. / 255, \n",
    "                                    horizontal_flip = True,\n",
    "                                    vertical_flip = False,\n",
    "                                    rotation_range = 20, # randomly rotate the image up to 20 degrees\n",
    "                                    shear_range = 0.1, # skew the sides of the images by up to 10 degrees \n",
    "                                    width_shift_range = 0.2, # shift the images left/right\n",
    "                                    height_shift_range = 0.2, # shift the images up/down TBD: needs more testing of values\n",
    "                                    brightness_range = [0.8, 1.2],\n",
    "                                    fill_mode = \"nearest\")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale= 1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale= 1. / 255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\"./datasets/train\",\n",
    "                                  batch_size = 64,\n",
    "                                  target_size=(image_size, image_size),\n",
    "                                  class_mode = \"binary\",\n",
    "                                  shuffle=True,\n",
    "                                  seed = 42,\n",
    "                                  color_mode= \"grayscale\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\"./datasets/test\",\n",
    "                                  batch_size = 64,\n",
    "                                  target_size=(image_size, image_size),\n",
    "                                  class_mode = \"binary\",\n",
    "                                  shuffle=True,\n",
    "                                  seed = 42,\n",
    "                                  color_mode= \"grayscale\")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\"./datasets/val\",\n",
    "                                  batch_size = 64,\n",
    "                                  target_size=(image_size, image_size),\n",
    "                                  class_mode = \"binary\",\n",
    "                                  shuffle=True,\n",
    "                                  seed = 42,\n",
    "                                  color_mode= \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = train_generator.class_indices\n",
    "\n",
    "print(class_labels)\n",
    "\n",
    "# Flip them around so we have \"index : value\"\n",
    "class_name = {}\n",
    "\n",
    "for value, index in class_labels.items():\n",
    "    class_name[index] = value\n",
    "\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer with minimal filters\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(image_size, image_size, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())  # Add batch normalization\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Fully connected block\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(64, activation='relu'))  # Additional dense layer\n",
    "#model.add(Dropout(0.5))  # Reduce overfitting\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(256, activation= 'relu')),\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation= 'relu')),\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor= \"val_loss\",\n",
    "                   verbose= 1,\n",
    "                   mode= \"min\",\n",
    "                   patience= 15)\n",
    "\n",
    "\n",
    "rate_reduction = ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
    "                        patience= 3,\n",
    "                        verbose= 1,\n",
    "                        factor= 0.8, # \"learning rate * factor\" after patience runs out\n",
    "                        min_lr= 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, \n",
    "            steps_per_epoch=82,\n",
    "            epochs=30,\n",
    "            callbacks=[early_stop, rate_reduction],\n",
    "            validation_data=valid_generator)\n",
    "\n",
    "\n",
    "\n",
    "val_eval = model.evaluate(valid_generator)\n",
    "test_eval = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss: {val_eval[0]}\")\n",
    "print(f\"Validation Accuarcy: {val_eval[1]}\")\n",
    "print(f\"Test Loss: {test_eval[0]}\")\n",
    "print(f\"Test Accuarcy: {test_eval[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
